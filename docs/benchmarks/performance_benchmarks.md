# Performance Benchmarks {#openvino_docs_performance_benchmarks}

@sphinxdirective

.. toctree::
   :maxdepth: 1
   :hidden:

   openvino_docs_performance_benchmarks_openvino
   openvino_docs_performance_benchmarks_ovms


@endsphinxdirective

The [Intel® Distribution of OpenVINO™ toolkit](https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html) helps accelerate deep learning inference across a variety of Intel® processors and accelerators.

Below benchmarks demonstrate the high performance gains on several public neural networks on multiple Intel® CPUs, GPUs and VPUs covering a broad performance range. The presetned data purpouse is to help you decide which hardware is the best choice for your applications and solutions, or to plan your AI workload on the already included Intel computing in your solutions.

Check the below links to review the benchmarking results for each option:

* [Intel® Distribution of OpenVINO™ toolkit Benchmark Results](performance_benchmarks_openvino.md)
* [OpenVINO™ Model Server Benchmark Results](performance_benchmarks_ovms.md)

Performance for a particular application can also be evaluated virtually using [Intel® DevCloud for the Edge](https://devcloud.intel.com/edge/), a remote development environment with access to Intel® hardware and the latest versions of the Intel® Distribution of the OpenVINO™ Toolkit. You can [learn more about DevCloud here](https://devcloud.intel.com/edge/get_started/devcloud/) or [Register here](https://inteliot.force.com/DevcloudForEdge/s/).
