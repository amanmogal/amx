# OpenVINO Workflow {#openvino_workflow}


@sphinxdirective

.. toctree::
   :maxdepth: 1
   :hidden:

   Model Preparation <openvino_docs_model_processing_introduction>
   Model Optimization and Compression <openvino_docs_model_optimization_guide>
   Running and Deploying Inference <openvino_docs_deployment_guide_introduction>
   

| :doc:`Model Preparation <openvino_docs_model_processing_introduction>`
| With model conversion API guide, you will learn to convert pre-trained models for use with OpenVINOâ„¢. You can use your own models or choose some from a broad selection in online databases, such as `TensorFlow Hub <https://tfhub.dev/>`__, `Hugging Face <https://huggingface.co/>`__, `Torchvision models <https://pytorch.org/hub/>`__..

| :doc:`Model Optimization and Compression <openvino_docs_model_optimization_guide>`
| In this section you will find out how to optimize a model to achieve better inference performance. It describes multiple optimization methods for both the training and post-training stages. 

| :doc:`Deployment <openvino_docs_deployment_guide_introduction>`
| This section explains the process of deploying your own inference application using either OpenVINO Runtime or OpenVINO Model Server. It describes how to run inference which is the most basic form of deployment and the quickest way of launching inference.

@endsphinxdirective