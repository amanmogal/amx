{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the worklflow of a simple image classification task.\n",
    "We will go through all the pipeline steps: downloading the model, generating the Intermediate Representation (IR) using the Model Optimizer, running inference in Python, and parsing and interpretating the output results.\n",
    "\n",
    "To demonstrate the scenario, we will use the pre-trained SquezeNet V1.1 Caffe\\* model. SqueezeNet is a pretty accurate and at the same time lightweight network. For more information about the model, please visit <a href=\"https://github.com/DeepScale/SqueezeNet/\">GitHub</a> page and refer to original <a href=\"https://arxiv.org/abs/1602.07360\">SqueezeNet paper</a>.\n",
    "\n",
    "Follow the steps to perform image classification with the SquezeNet V1.1 model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Download the model files:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"Downloading deploy.protxt ...\"\n",
    "if [ -f deploy.prototxt ]; then \n",
    "    echo \"deploy.protxt file already exists. Downloading skipped\"\n",
    "else\n",
    "    wget https://raw.githubusercontent.com/DeepScale/SqueezeNet/a47b6f13d30985279789d08053d37013d67d131b/SqueezeNet_v1.1/deploy.prototxt -q\n",
    "    echo \"Finished!\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "! echo \"Downloading squeezenet_v1.1.caffemodel ...\"\n",
    "if [ -f squeezenet_v1.1.caffemodel ]; then\n",
    "    echo \"squeezenet_v1.1.caffemodel file already exists. Download skipped\"\n",
    "else\n",
    "    wget https://github.com/DeepScale/SqueezeNet/raw/a47b6f13d30985279789d08053d37013d67d131b/SqueezeNet_v1.1/squeezenet_v1.1.caffemodel -q\n",
    "    echo \"Finished!\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the following command to see the model files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `deploy.prototxt` contains the network toplogy description in text format. \n",
    "* `squeezenet_v1.1.caffemodel` contains weights for all network layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Optimize and convert the model from intial Caffe representation to the IR representation, which is required for scoring the model using Inference Engine. To convert and optimize the model, use the Model Optimizer command line tool.**\n",
    "\n",
    "To locate Model Optimizer scripts, specify the path to the Model Optimizer root directory in the `MO_ROOT` variable in the cell bellow and then run it (If you use the installed OpenVINO&trade; package, you can find the Model Optimizer in `<INSTALLATION_ROOT_DIR>/deployment_tools/model_optimizer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MO_ROOT=/localdisk/repos/model-optimizer-tensorflow/\n",
    "echo $MO_ROOT\n",
    "python3 $MO_ROOT/mo.py --input_model squeezenet_v1.1.caffemodel --input_proto deploy.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Now, you have the SqueezeNet model converted to the IR, and you can infer it.**\n",
    "\n",
    "a. First, import required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "import numpy as np\n",
    "import cv2\n",
    "import logging as log\n",
    "from time import time\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Initialize required constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging format\n",
    "log.basicConfig(format=\"[ %(levelname)s ] %(message)s\", level=log.INFO, stream=sys.stdout)\n",
    "\n",
    "# Path to IR model files\n",
    "MODEL_XML = \"./squeezenet_v1.1.xml\"\n",
    "MODEL_BIN = \"./squeezenet_v1.1.bin\"\n",
    "\n",
    "# Target device to run inference\n",
    "TARGET_DEVICE = \"CPU\"\n",
    "\n",
    "# Folder with input images for the model\n",
    "IMAGES_FOLDER = \"./images\"\n",
    "\n",
    "# File containing information about classes names \n",
    "LABELS_FILE = \"./image_net_synset.txt\"\n",
    "\n",
    "# Number of top prediction results to parse\n",
    "NTOP = 5\n",
    "\n",
    "# Required batch size - number of images which will be processed in parallel\n",
    "BATCH = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Create a plugin instance for the specified target device  \n",
    "d. Read the IR files and create an `IENEtwork` instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plugin = IEPlugin(TARGET_DEVICE)\n",
    "net = IENetwork(model=MODEL_XML, weights=MODEL_BIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Set the network batch size to the constatns specified above. \n",
    "\n",
    "Batch size is an \"amount\" of input data that will be infered in parallel. In this cases it is a number of images, which will be classified in parallel. \n",
    "\n",
    "You can set the network batch size using one of the following options:\n",
    "1. On the IR generation stage, run the Model Optimizer with `-b` command line option. For example, to generate the IR with batch size equal to 4, add `-b 4` to Model Optimizer command line options. By default, it takes the batch size from the original network in framework representation (usually, it is equal to 1, but in this case, the original Caffe model is provided with the batch size equal to 10). \n",
    "2. Use Inference Engine after reading IR. We will use this option.\n",
    "\n",
    "To set the batch size with the Inference Engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"Current network batch size is {}, will be changed to {}\".format(net.batch_size, BATCH))\n",
    "net.batch_size = BATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. After setting batch size, you can get required information about network input layers.\n",
    "To preprocess input images, you need to know input layer shape.\n",
    "\n",
    "`inputs` property of `IENetwork` returns the dicitonary with input layer names and `InputInfo` objects, which contain information about an input layer including its shape.\n",
    "\n",
    "SqueezeNet is a single-input toplogy, so to get the input layer name and its shape, you can get the first item from the `inputs` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = next(iter(net.inputs))\n",
    "n,c,h,w = net.inputs[input_layer].shape\n",
    "layout = net.inputs[input_layer].layout\n",
    "log.info(\"Network input layer {} has shape {} and layout {}\".format(input_layer, (n,c,h,w), layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do the shape and layout mean?  \n",
    "Layout will helps to interprete the shape dimsesnions meaning. \n",
    "\n",
    "`NCHW` input layer layout means:\n",
    "* the fisrt dimension of an input data is a batch of **N** images processed in parallel \n",
    "* the second dimension is a numnber of **C**hannels expected in the input images\n",
    "* the third and the forth are a spatial dimensions - **H**eight and **W**idth of an input image\n",
    "\n",
    "Our shapes means that the network expects four 3-channel images running in parallel with size 227x227."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Read and preprocess input images.\n",
    "\n",
    "For it, go to `IMAGES_FOLDER`, find all `.bmp` files, and take four images for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_pattern = os.path.join(IMAGES_FOLDER, \"*.bmp\")\n",
    "images = glob.glob(search_pattern)[:BATCH]\n",
    "log.info(\"Input images:\\n {}\".format(\"\\n\".join(images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can read and preprocess the image files and create an array with input blob data.\n",
    "\n",
    "For preprocessing, you must do the following:\n",
    "1. Resize the images to fit the HxW input dimenstions.\n",
    "2. Transpose the HWC layout.\n",
    "\n",
    "Transposing is tricky and not really obvious.\n",
    "As you alredy saw above, the network has the `NCHW` layout, so each input image should be in `CHW` format. But by deafult, OpenCV\\* reads images in the `HWC` format. That is why you have to swap the axes using the `numpy.transpose()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.ndarray(shape=(n, c, h, w))\n",
    "orig_images = [] # Will be used to show image in notebook\n",
    "for i, img in enumerate(images):\n",
    "    image = cv2.imread(img)\n",
    "    orig_images.append(image)\n",
    "    if image.shape[:-1] != (h, w):\n",
    "        log.warning(\"Image {} is resized from {} to {}\".format(img, image.shape[:-1], (h, w)))\n",
    "        image = cv2.resize(image, (w, h))\n",
    "    image = image.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "    input_data[i] = image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Infer the model model to classify input images:\n",
    "\n",
    "1. Load the `IENetwork` object to the plugin to create `ExectuableNEtwork` object.    \n",
    "2. Start inference using the `infer()` function specifying dictionary with input layer name and prepared data as an argument for the function.     \n",
    "3. Measure inference time in miliseconds and calculate throughput metric in frames-per-second (FPS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_net = plugin.load(net)\n",
    "t0 = time()\n",
    "res_map = exec_net.infer({input_layer: input_data})\n",
    "inf_time = (time() - t0) * 1000 \n",
    "fps = BATCH * inf_time \n",
    "log.info(\"Inference time: {} ms.\".format(inf_time))\n",
    "log.info(\"Throughput: {} fps.\".format(fps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. After the inference, you need to parse and interpretate the inference results.**\n",
    "\n",
    "First, you need to see the shape of the network output layer. It can be done in similar way as for the inputs, but here you need to call `outputs` property of `IENetwork` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = next(iter(net.outputs))\n",
    "n,c,h,w = net.outputs[output_layer].shape\n",
    "layout = net.outputs[output_layer].layout\n",
    "log.info(\"Network output layer {} has shape {} and layout {}\".format(output_layer, (n,c,h,w), layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not a common case for classification netowrks to have output layer with *NCHW* layout. Usually, it is just *NC*. However, in this case, the last two dimensions are just a feature of the network and do not have much sense. Ignore them as you will remove  them on the final parsing stage. \n",
    "\n",
    "What are the first and second dimensions of the output layer?    \n",
    "* The first dimension is a batch. We precoessed four images, and the prediction result for a particular image is stored in the first dimension of the output array. For example, prediction results for the third image is `res[2]` (since numeration starts from 0).\n",
    "* The second dimension is an array with normalized probabilities (from 0 to 1) for each class. This network is trained using the <a href=\"http://image-net.org/index\">ImageNet</a> dataset with 1000 classes. Each `n`-th value in the output data for a certain image represent the probability of the image belonging to the `n`-th class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse the output results:\n",
    "\n",
    "a. Read the `LABELS_FILE`, which maps the class ID to human-readable class names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LABELS_FILE, 'r') as f:\n",
    "    labels_map = [x.split(sep=' ', maxsplit=1)[-1].strip() for x in f]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Parse the output array with prediction results. The parsing algorith is the following:\n",
    "0. Squeeze the last two \"extra\" dimensions of the output data.\n",
    "1. Iterate over all batches.\n",
    "2. Sort the probabilities vector descendingly to get `NTOP` classes with the highest probabilities (by default, the `numpy.argsort` sorts the data in the ascending order, but using the array slicing `[::-1]`, you can reverse the data order).\n",
    "3. Map the `NTOP` probabilities to the corresponding labeles in `labeles_map`.\n",
    "\n",
    "For the vizualization, you also need to store top-1 class and probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_res = [] # will be used for the visualization\n",
    "res = np.squeeze(res_map[output_layer])\n",
    "log.info(\"Top {} results: \".format(NTOP))\n",
    "for i, probs in enumerate(res):\n",
    "    top_ind = np.argsort(probs)[-NTOP:][::-1]\n",
    "    print(\"Image {}\".format(images[i]))\n",
    "    top1_ind = top_ind[0]\n",
    "    top1_res.append((labels_map[top1_ind], probs[top1_ind]))\n",
    "    for id in top_ind:\n",
    "        print(\"label: {}   probability: {:.2f}% \".format(labels_map[id], probs[id] * 100))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above prints the results as plain text.   \n",
    "You can also use OpenCV\\* to visualize the results using the `orig_images` and `top1_res` variables, which you created during images reading and results parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "for i, img in enumerate(orig_images):\n",
    "    label_str = \"{}\".format(top1_res[i][0].split(',')[0])\n",
    "    prob_str = \"{:.2f}%\".format(top1_res[i][1])\n",
    "    cv2.putText(img, label_str, (5, 15), cv2.FONT_HERSHEY_COMPLEX, 0.6, (220,100,10), 1)\n",
    "    cv2.putText(img, prob_str, (5, 35), cv2.FONT_HERSHEY_COMPLEX, 0.6, (220,100,10), 1)\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # We have to convert colors, because matplotlib expects an image in RGB color format \n",
    "    # but by default, the OpenCV read images in BRG format\n",
    "    im_to_show = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(im_to_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
