Benchmark results for handwritten-simplified-chinese-recognition-0001 FP16 latency CPU:
[Step 1/11] Parsing and validating input arguments
[ INFO ] Parsing input parameters
[Step 2/11] Loading OpenVINO Runtime
[ INFO ] OpenVINO:
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] Device info:
[ INFO ] CPU
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] 
[Step 3/11] Setting device configuration
[Step 4/11] Reading model files
[ INFO ] Loading model files
[ INFO ] Read model took 37.47 ms
[ INFO ] Original model I/O parameters:
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 5/11] Resizing model to match image sizes and given batch
[ INFO ] Model batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 278.64 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] Model:
[ INFO ]   NETWORK_NAME: torch-jit-export
[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1
[ INFO ]   NUM_STREAMS: 1
[ INFO ]   AFFINITY: Affinity.HYBRID_AWARE
[ INFO ]   INFERENCE_NUM_THREADS: 6
[ INFO ]   PERF_COUNT: False
[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>
[ INFO ]   PERFORMANCE_HINT: PerformanceMode.LATENCY
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0
[Step 9/11] Creating infer requests and preparing input tensors
[ WARNING ] No input files were given for input 'actual_input'!. This input will be filled with random values!
[ INFO ] Fill input 'actual_input' with random values 
[Step 10/11] Measuring performance (Start inference asynchronously, 1 inference requests, limits: 60000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 214.41 ms
[Step 11/11] Dumping statistics report
[ INFO ] Count:            233 iterations
[ INFO ] Duration:         60366.59 ms
[ INFO ] Latency:
[ INFO ]    Median:        258.17 ms
[ INFO ]    Average:       258.98 ms
[ INFO ]    Min:           239.77 ms
[ INFO ]    Max:           334.48 ms
[ INFO ] Throughput:   3.86 FPS

--------------------

Benchmark results for handwritten-simplified-chinese-recognition-0001 FP16 latency GPU:
[Step 1/11] Parsing and validating input arguments
[ INFO ] Parsing input parameters
[Step 2/11] Loading OpenVINO Runtime
[ INFO ] OpenVINO:
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] Device info:
[ INFO ] GPU
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] 
[Step 3/11] Setting device configuration
[Step 4/11] Reading model files
[ INFO ] Loading model files
[ INFO ] Read model took 19.69 ms
[ INFO ] Original model I/O parameters:
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 5/11] Resizing model to match image sizes and given batch
[ INFO ] Model batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 1535.20 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] Model:
[ INFO ]   NETWORK_NAME: torch-jit-export
[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1
[ INFO ]   PERF_COUNT: False
[ INFO ]   MODEL_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_HOST_TASK_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_QUEUE_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_QUEUE_THROTTLE: Priority.MEDIUM
[ INFO ]   GPU_ENABLE_LOOP_UNROLLING: True
[ INFO ]   CACHE_DIR: 
[ INFO ]   PERFORMANCE_HINT: PerformanceMode.LATENCY
[ INFO ]   COMPILATION_NUM_THREADS: 20
[ INFO ]   NUM_STREAMS: 1
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0
[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'undefined'>
[ INFO ]   DEVICE_ID: 0
[Step 9/11] Creating infer requests and preparing input tensors
[ WARNING ] No input files were given for input 'actual_input'!. This input will be filled with random values!
[ INFO ] Fill input 'actual_input' with random values 
[Step 10/11] Measuring performance (Start inference asynchronously, 1 inference requests, limits: 60000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 50.83 ms
[Step 11/11] Dumping statistics report
[ INFO ] Count:            1064 iterations
[ INFO ] Duration:         60058.76 ms
[ INFO ] Latency:
[ INFO ]    Median:        56.37 ms
[ INFO ]    Average:       56.28 ms
[ INFO ]    Min:           50.44 ms
[ INFO ]    Max:           58.83 ms
[ INFO ] Throughput:   17.72 FPS

--------------------

Benchmark results for handwritten-simplified-chinese-recognition-0001 FP16-INT8 latency CPU:
[Step 1/11] Parsing and validating input arguments
[ INFO ] Parsing input parameters
[Step 2/11] Loading OpenVINO Runtime
[ INFO ] OpenVINO:
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] Device info:
[ INFO ] CPU
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] 
[Step 3/11] Setting device configuration
[Step 4/11] Reading model files
[ INFO ] Loading model files
[ INFO ] Read model took 36.58 ms
[ INFO ] Original model I/O parameters:
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 5/11] Resizing model to match image sizes and given batch
[ INFO ] Model batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 89.45 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] Model:
[ INFO ]   NETWORK_NAME: torch-jit-export
[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1
[ INFO ]   NUM_STREAMS: 1
[ INFO ]   AFFINITY: Affinity.HYBRID_AWARE
[ INFO ]   INFERENCE_NUM_THREADS: 6
[ INFO ]   PERF_COUNT: False
[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>
[ INFO ]   PERFORMANCE_HINT: PerformanceMode.LATENCY
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0
[Step 9/11] Creating infer requests and preparing input tensors
[ WARNING ] No input files were given for input 'actual_input'!. This input will be filled with random values!
[ INFO ] Fill input 'actual_input' with random values 
[Step 10/11] Measuring performance (Start inference asynchronously, 1 inference requests, limits: 60000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 62.98 ms
[Step 11/11] Dumping statistics report
[ INFO ] Count:            790 iterations
[ INFO ] Duration:         60133.99 ms
[ INFO ] Latency:
[ INFO ]    Median:        75.76 ms
[ INFO ]    Average:       75.98 ms
[ INFO ]    Min:           67.22 ms
[ INFO ]    Max:           112.86 ms
[ INFO ] Throughput:   13.14 FPS

--------------------

Benchmark results for handwritten-simplified-chinese-recognition-0001 FP16-INT8 latency GPU:
[Step 1/11] Parsing and validating input arguments
[ INFO ] Parsing input parameters
[Step 2/11] Loading OpenVINO Runtime
[ INFO ] OpenVINO:
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] Device info:
[ INFO ] GPU
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] 
[Step 3/11] Setting device configuration
[Step 4/11] Reading model files
[ INFO ] Loading model files
[ INFO ] Read model took 16.91 ms
[ INFO ] Original model I/O parameters:
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 5/11] Resizing model to match image sizes and given batch
[ INFO ] Model batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 1642.23 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] Model:
[ INFO ]   NETWORK_NAME: torch-jit-export
[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1
[ INFO ]   PERF_COUNT: False
[ INFO ]   MODEL_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_HOST_TASK_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_QUEUE_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_QUEUE_THROTTLE: Priority.MEDIUM
[ INFO ]   GPU_ENABLE_LOOP_UNROLLING: True
[ INFO ]   CACHE_DIR: 
[ INFO ]   PERFORMANCE_HINT: PerformanceMode.LATENCY
[ INFO ]   COMPILATION_NUM_THREADS: 20
[ INFO ]   NUM_STREAMS: 1
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0
[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'undefined'>
[ INFO ]   DEVICE_ID: 0
[Step 9/11] Creating infer requests and preparing input tensors
[ WARNING ] No input files were given for input 'actual_input'!. This input will be filled with random values!
[ INFO ] Fill input 'actual_input' with random values 
[Step 10/11] Measuring performance (Start inference asynchronously, 1 inference requests, limits: 60000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 35.18 ms
[Step 11/11] Dumping statistics report
[ INFO ] Count:            1666 iterations
[ INFO ] Duration:         60048.01 ms
[ INFO ] Latency:
[ INFO ]    Median:        35.83 ms
[ INFO ]    Average:       35.87 ms
[ INFO ]    Min:           34.46 ms
[ INFO ]    Max:           37.36 ms
[ INFO ] Throughput:   27.74 FPS

--------------------

Benchmark results for handwritten-simplified-chinese-recognition-0001 FP32 latency CPU:
[Step 1/11] Parsing and validating input arguments
[ INFO ] Parsing input parameters
[Step 2/11] Loading OpenVINO Runtime
[ INFO ] OpenVINO:
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] Device info:
[ INFO ] CPU
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] 
[Step 3/11] Setting device configuration
[Step 4/11] Reading model files
[ INFO ] Loading model files
[ INFO ] Read model took 58.98 ms
[ INFO ] Original model I/O parameters:
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 5/11] Resizing model to match image sizes and given batch
[ INFO ] Model batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 48.41 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] Model:
[ INFO ]   NETWORK_NAME: torch-jit-export
[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1
[ INFO ]   NUM_STREAMS: 1
[ INFO ]   AFFINITY: Affinity.HYBRID_AWARE
[ INFO ]   INFERENCE_NUM_THREADS: 6
[ INFO ]   PERF_COUNT: False
[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>
[ INFO ]   PERFORMANCE_HINT: PerformanceMode.LATENCY
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0
[Step 9/11] Creating infer requests and preparing input tensors
[ WARNING ] No input files were given for input 'actual_input'!. This input will be filled with random values!
[ INFO ] Fill input 'actual_input' with random values 
[Step 10/11] Measuring performance (Start inference asynchronously, 1 inference requests, limits: 60000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 253.93 ms
[Step 11/11] Dumping statistics report
[ INFO ] Count:            229 iterations
[ INFO ] Duration:         60337.17 ms
[ INFO ] Latency:
[ INFO ]    Median:        261.29 ms
[ INFO ]    Average:       263.36 ms
[ INFO ]    Min:           240.53 ms
[ INFO ]    Max:           364.09 ms
[ INFO ] Throughput:   3.80 FPS

--------------------

Benchmark results for handwritten-simplified-chinese-recognition-0001 FP32 latency GPU:
[Step 1/11] Parsing and validating input arguments
[ INFO ] Parsing input parameters
[Step 2/11] Loading OpenVINO Runtime
[ INFO ] OpenVINO:
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] Device info:
[ INFO ] GPU
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] 
[Step 3/11] Setting device configuration
[Step 4/11] Reading model files
[ INFO ] Loading model files
[ INFO ] Read model took 31.27 ms
[ INFO ] Original model I/O parameters:
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 5/11] Resizing model to match image sizes and given batch
[ INFO ] Model batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 1670.41 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] Model:
[ INFO ]   NETWORK_NAME: torch-jit-export
[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1
[ INFO ]   PERF_COUNT: False
[ INFO ]   MODEL_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_HOST_TASK_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_QUEUE_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_QUEUE_THROTTLE: Priority.MEDIUM
[ INFO ]   GPU_ENABLE_LOOP_UNROLLING: True
[ INFO ]   CACHE_DIR: 
[ INFO ]   PERFORMANCE_HINT: PerformanceMode.LATENCY
[ INFO ]   COMPILATION_NUM_THREADS: 20
[ INFO ]   NUM_STREAMS: 1
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0
[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'undefined'>
[ INFO ]   DEVICE_ID: 0
[Step 9/11] Creating infer requests and preparing input tensors
[ WARNING ] No input files were given for input 'actual_input'!. This input will be filled with random values!
[ INFO ] Fill input 'actual_input' with random values 
[Step 10/11] Measuring performance (Start inference asynchronously, 1 inference requests, limits: 60000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 93.80 ms
[Step 11/11] Dumping statistics report
[ INFO ] Count:            570 iterations
[ INFO ] Duration:         60135.82 ms
[ INFO ] Latency:
[ INFO ]    Median:        105.21 ms
[ INFO ]    Average:       105.33 ms
[ INFO ]    Min:           93.12 ms
[ INFO ]    Max:           107.72 ms
[ INFO ] Throughput:   9.48 FPS

--------------------

Benchmark results for handwritten-simplified-chinese-recognition-0001 FP16 throughput CPU:
[Step 1/11] Parsing and validating input arguments
[ INFO ] Parsing input parameters
[Step 2/11] Loading OpenVINO Runtime
[ INFO ] OpenVINO:
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] Device info:
[ INFO ] CPU
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] 
[Step 3/11] Setting device configuration
[Step 4/11] Reading model files
[ INFO ] Loading model files
[ INFO ] Read model took 22.95 ms
[ INFO ] Original model I/O parameters:
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 5/11] Resizing model to match image sizes and given batch
[ INFO ] Model batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 317.22 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] Model:
[ INFO ]   NETWORK_NAME: torch-jit-export
[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 6
[ INFO ]   NUM_STREAMS: 6
[ INFO ]   AFFINITY: Affinity.HYBRID_AWARE
[ INFO ]   INFERENCE_NUM_THREADS: 18
[ INFO ]   PERF_COUNT: False
[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>
[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0
[Step 9/11] Creating infer requests and preparing input tensors
[ WARNING ] No input files were given for input 'actual_input'!. This input will be filled with random values!
[ INFO ] Fill input 'actual_input' with random values 
[Step 10/11] Measuring performance (Start inference asynchronously, 6 inference requests, limits: 60000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 364.92 ms
[Step 11/11] Dumping statistics report
[ INFO ] Count:            258 iterations
[ INFO ] Duration:         61338.98 ms
[ INFO ] Latency:
[ INFO ]    Median:        1300.03 ms
[ INFO ]    Average:       1418.03 ms
[ INFO ]    Min:           707.99 ms
[ INFO ]    Max:           1842.35 ms
[ INFO ] Throughput:   4.21 FPS

--------------------

Benchmark results for handwritten-simplified-chinese-recognition-0001 FP16 throughput GPU:
[Step 1/11] Parsing and validating input arguments
[ INFO ] Parsing input parameters
[Step 2/11] Loading OpenVINO Runtime
[ INFO ] OpenVINO:
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] Device info:
[ INFO ] GPU
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] 
[Step 3/11] Setting device configuration
[Step 4/11] Reading model files
[ INFO ] Loading model files
[ INFO ] Read model took 20.04 ms
[ INFO ] Original model I/O parameters:
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 5/11] Resizing model to match image sizes and given batch
[ INFO ] Model batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 1570.10 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] Model:
[ INFO ]   NETWORK_NAME: torch-jit-export
[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 4
[ INFO ]   PERF_COUNT: False
[ INFO ]   MODEL_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_HOST_TASK_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_QUEUE_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_QUEUE_THROTTLE: Priority.MEDIUM
[ INFO ]   GPU_ENABLE_LOOP_UNROLLING: True
[ INFO ]   CACHE_DIR: 
[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT
[ INFO ]   COMPILATION_NUM_THREADS: 20
[ INFO ]   NUM_STREAMS: 2
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0
[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'undefined'>
[ INFO ]   DEVICE_ID: 0
[Step 9/11] Creating infer requests and preparing input tensors
[ WARNING ] No input files were given for input 'actual_input'!. This input will be filled with random values!
[ INFO ] Fill input 'actual_input' with random values 
[Step 10/11] Measuring performance (Start inference asynchronously, 4 inference requests, limits: 60000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 51.04 ms
[Step 11/11] Dumping statistics report
[ INFO ] Count:            1044 iterations
[ INFO ] Duration:         60259.67 ms
[ INFO ] Latency:
[ INFO ]    Median:        230.84 ms
[ INFO ]    Average:       230.34 ms
[ INFO ]    Min:           91.72 ms
[ INFO ]    Max:           232.70 ms
[ INFO ] Throughput:   17.33 FPS

--------------------

Benchmark results for handwritten-simplified-chinese-recognition-0001 FP16-INT8 throughput CPU:
[Step 1/11] Parsing and validating input arguments
[ INFO ] Parsing input parameters
[Step 2/11] Loading OpenVINO Runtime
[ INFO ] OpenVINO:
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] Device info:
[ INFO ] CPU
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] 
[Step 3/11] Setting device configuration
[Step 4/11] Reading model files
[ INFO ] Loading model files
[ INFO ] Read model took 19.38 ms
[ INFO ] Original model I/O parameters:
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 5/11] Resizing model to match image sizes and given batch
[ INFO ] Model batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 107.53 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] Model:
[ INFO ]   NETWORK_NAME: torch-jit-export
[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 6
[ INFO ]   NUM_STREAMS: 6
[ INFO ]   AFFINITY: Affinity.HYBRID_AWARE
[ INFO ]   INFERENCE_NUM_THREADS: 18
[ INFO ]   PERF_COUNT: False
[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>
[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0
[Step 9/11] Creating infer requests and preparing input tensors
[ WARNING ] No input files were given for input 'actual_input'!. This input will be filled with random values!
[ INFO ] Fill input 'actual_input' with random values 
[Step 10/11] Measuring performance (Start inference asynchronously, 6 inference requests, limits: 60000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 109.42 ms
[Step 11/11] Dumping statistics report
[ INFO ] Count:            864 iterations
[ INFO ] Duration:         60558.18 ms
[ INFO ] Latency:
[ INFO ]    Median:        331.70 ms
[ INFO ]    Average:       419.37 ms
[ INFO ]    Min:           255.02 ms
[ INFO ]    Max:           908.59 ms
[ INFO ] Throughput:   14.27 FPS

--------------------

Benchmark results for handwritten-simplified-chinese-recognition-0001 FP16-INT8 throughput GPU:
[Step 1/11] Parsing and validating input arguments
[ INFO ] Parsing input parameters
[Step 2/11] Loading OpenVINO Runtime
[ INFO ] OpenVINO:
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] Device info:
[ INFO ] GPU
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] 
[Step 3/11] Setting device configuration
[Step 4/11] Reading model files
[ INFO ] Loading model files
[ INFO ] Read model took 13.86 ms
[ INFO ] Original model I/O parameters:
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 5/11] Resizing model to match image sizes and given batch
[ INFO ] Model batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 1532.06 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] Model:
[ INFO ]   NETWORK_NAME: torch-jit-export
[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 4
[ INFO ]   PERF_COUNT: False
[ INFO ]   MODEL_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_HOST_TASK_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_QUEUE_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_QUEUE_THROTTLE: Priority.MEDIUM
[ INFO ]   GPU_ENABLE_LOOP_UNROLLING: True
[ INFO ]   CACHE_DIR: 
[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT
[ INFO ]   COMPILATION_NUM_THREADS: 20
[ INFO ]   NUM_STREAMS: 2
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0
[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'undefined'>
[ INFO ]   DEVICE_ID: 0
[Step 9/11] Creating infer requests and preparing input tensors
[ WARNING ] No input files were given for input 'actual_input'!. This input will be filled with random values!
[ INFO ] Fill input 'actual_input' with random values 
[Step 10/11] Measuring performance (Start inference asynchronously, 4 inference requests, limits: 60000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 35.61 ms
[Step 11/11] Dumping statistics report
[ INFO ] Count:            1624 iterations
[ INFO ] Duration:         60206.08 ms
[ INFO ] Latency:
[ INFO ]    Median:        147.85 ms
[ INFO ]    Average:       148.02 ms
[ INFO ]    Min:           69.65 ms
[ INFO ]    Max:           152.81 ms
[ INFO ] Throughput:   26.97 FPS

--------------------

Benchmark results for handwritten-simplified-chinese-recognition-0001 FP32 throughput CPU:
[Step 1/11] Parsing and validating input arguments
[ INFO ] Parsing input parameters
[Step 2/11] Loading OpenVINO Runtime
[ INFO ] OpenVINO:
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] Device info:
[ INFO ] CPU
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] 
[Step 3/11] Setting device configuration
[Step 4/11] Reading model files
[ INFO ] Loading model files
[ INFO ] Read model took 46.71 ms
[ INFO ] Original model I/O parameters:
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 5/11] Resizing model to match image sizes and given batch
[ INFO ] Model batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 91.96 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] Model:
[ INFO ]   NETWORK_NAME: torch-jit-export
[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 6
[ INFO ]   NUM_STREAMS: 6
[ INFO ]   AFFINITY: Affinity.HYBRID_AWARE
[ INFO ]   INFERENCE_NUM_THREADS: 18
[ INFO ]   PERF_COUNT: False
[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>
[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0
[Step 9/11] Creating infer requests and preparing input tensors
[ WARNING ] No input files were given for input 'actual_input'!. This input will be filled with random values!
[ INFO ] Fill input 'actual_input' with random values 
[Step 10/11] Measuring performance (Start inference asynchronously, 6 inference requests, limits: 60000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 380.99 ms
[Step 11/11] Dumping statistics report
[ INFO ] Count:            258 iterations
[ INFO ] Duration:         62028.69 ms
[ INFO ] Latency:
[ INFO ]    Median:        1310.32 ms
[ INFO ]    Average:       1438.52 ms
[ INFO ]    Min:           1154.72 ms
[ INFO ]    Max:           1839.98 ms
[ INFO ] Throughput:   4.16 FPS

--------------------

Benchmark results for handwritten-simplified-chinese-recognition-0001 FP32 throughput GPU:
[Step 1/11] Parsing and validating input arguments
[ INFO ] Parsing input parameters
[Step 2/11] Loading OpenVINO Runtime
[ INFO ] OpenVINO:
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] Device info:
[ INFO ] GPU
[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3
[ INFO ] 
[ INFO ] 
[Step 3/11] Setting device configuration
[Step 4/11] Reading model files
[ INFO ] Loading model files
[ INFO ] Read model took 32.09 ms
[ INFO ] Original model I/O parameters:
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 5/11] Resizing model to match image sizes and given batch
[ INFO ] Model batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model inputs:
[ INFO ]     actual_input (node: actual_input) : f32 / [N,C,H,W] / [1,1,96,2000]
[ INFO ] Model outputs:
[ INFO ]     output (node: output) : f32 / [...] / [186,1,4059]
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 1731.60 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] Model:
[ INFO ]   NETWORK_NAME: torch-jit-export
[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 4
[ INFO ]   PERF_COUNT: False
[ INFO ]   MODEL_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_HOST_TASK_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_QUEUE_PRIORITY: Priority.MEDIUM
[ INFO ]   GPU_QUEUE_THROTTLE: Priority.MEDIUM
[ INFO ]   GPU_ENABLE_LOOP_UNROLLING: True
[ INFO ]   CACHE_DIR: 
[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT
[ INFO ]   COMPILATION_NUM_THREADS: 20
[ INFO ]   NUM_STREAMS: 2
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0
[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'undefined'>
[ INFO ]   DEVICE_ID: 0
[Step 9/11] Creating infer requests and preparing input tensors
[ WARNING ] No input files were given for input 'actual_input'!. This input will be filled with random values!
[ INFO ] Fill input 'actual_input' with random values 
[Step 10/11] Measuring performance (Start inference asynchronously, 4 inference requests, limits: 60000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 93.69 ms
[Step 11/11] Dumping statistics report
[ INFO ] Count:            560 iterations
[ INFO ] Duration:         60648.85 ms
[ INFO ] Latency:
[ INFO ]    Median:        433.17 ms
[ INFO ]    Average:       431.88 ms
[ INFO ]    Min:           183.83 ms
[ INFO ]    Max:           441.29 ms
[ INFO ] Throughput:   9.23 FPS

--------------------

