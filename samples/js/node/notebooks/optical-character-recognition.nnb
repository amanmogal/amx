{
    "cells": [
        {
            "language": "markdown",
            "source": [
                "# Optical Character Recognition with OpenVINOâ„¢"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "#### This tutorial demonstrates how to perform optical character recognition (OCR) with OpenVINO models"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "# Imports"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "const { addon: ov } = require('openvino-node');\nconst { display } = require('node-kernel');\n\nconst Image = require('../image');\nconst { transform, argMax, setShape } = require('../helpers.js');\n"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "# Load a Detection Model"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "const detModelXMLPath = '../../assets/models/horizontal-text-detection-0001.xml';\n\n// Initialize OpenVINO core and load the detection model\nconst core = new ov.Core();\nconst detModel = await core.readModel(detModelXMLPath);\nconst detCompiledModel = await core.compileModel(detModel, 'AUTO');\nconst detInputLayer = detCompiledModel.input(0);\nconst detOutputLayer = detCompiledModel.output('boxes');\n"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "# Prepare Image for Inference"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "const imagePath = '../../assets/images/intel_rnb.jpg';\nconst img = await Image.load(imagePath);\nimg.display(display);\n\n// Resize the image to meet network input size\nconst [detInputHeight, detInputWidth] = detInputLayer.shape.slice(2);\nconst resizedImg = img.resize(detInputWidth, detInputHeight);\n\n// Prepare input tensor\nconst inputImageTransformedData = transform(\n  resizedImg.rgb,\n  { width: detInputWidth, height: detInputHeight },\n  [0, 1, 2],\n);\nconst tensorData = new Float32Array(inputImageTransformedData);\nconst tensor = new ov.Tensor(ov.element.f32, detInputLayer.shape, tensorData);\n"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "## Run inference on the detection model"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "const detInferRequest = detCompiledModel.createInferRequest();\nconst detResult = await detInferRequest.inferAsync([tensor]);\n"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "# Load Text Recognition Model"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "const recModelXMLPath = '../../assets/models/text-recognition-resnet-fc.xml';\n\n// Load the recognition model and prepare the inference request\nconst recModel = await core.readModel(recModelXMLPath);\nconst recModelCompiled = await core.compileModel(recModel, 'AUTO');\nconst recInferRequest = recModelCompiled.createInferRequest();\n"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "# Define Post-Processing Functions"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "async function performTextRecognition(model, inferenceRequest, img) {\n  const inputLayerShape = model.input(0).shape;\n  const outputLayer = model.output(0);\n\n  const [,, inputHeight, inputWidth] = inputLayerShape;\n  const resizedImg = img.resize(inputWidth, inputHeight);\n\n  // Convert image to grayscale and create tensor\n  const tensor = new ov.Tensor(\n    ov.element.f32,\n    inputLayerShape,\n    new Float32Array(resizedImg.grayscale),\n  );\n\n  const result = await inferenceRequest.inferAsync([tensor]);\n  const recognitionResults = extractRecognitionResults(result[outputLayer]);\n  const annotation = parseAnnotations(recognitionResults);\n\n  return annotation;\n}\n\n// Function to extract bounding boxes from the model output\nfunction extractBoundingBoxes(output) {\n  const { data: boxes } = output;\n  const foldingCoefficient = 5;\n  const numberOfBoxes = boxes.length / foldingCoefficient;\n\n  return setShape(boxes, [numberOfBoxes, foldingCoefficient]);\n}\n\n// Function to adjust bounding box coordinates by a given ratio\nfunction multiplyByRatio(ratioX, ratioY, box) {\n  const scaleShape = (shape, idx) => {\n    const position = idx % 2\n      ? Math.max(shape * ratioY, 10)\n      : shape * ratioX;\n\n    return Math.floor(position);\n  }\n\n  return box.map(scaleShape);\n}\n\n// Function to extract recognition results from the model output\nfunction extractRecognitionResults(output) {\n  const outputData = output.getData();\n  const outputShape = output.getShape();\n  const [, height, width] = outputShape;\n\n  return setShape(outputData, [height, width]);\n}\n\n// Function to parse annotations from the recognition results\nfunction parseAnnotations(recognitionResults) {\n  const letters = '~0123456789abcdefghijklmnopqrstuvwxyz';\n  const annotation = [];\n\n  for (const row of recognitionResults) {\n    const letterIndex = argMax(row);\n    const parsedLetter = letters[letterIndex];\n\n    // Stop if end character is encountered\n    if (parsedLetter === letters[0]) break;\n\n    annotation.push(parsedLetter);\n  }\n\n  return annotation.join('');\n}\n\n// Takes original image and bounding boxes with annotations\n// and returns the image with annotations\nasync function putAnnotationsOnImage(img, boxesWithAnnotations, options) {\n  const defaultOptions = { threshold: 0.3, confLabels: true };\n  const { threshold, confLabels } = Object.assign(defaultOptions, options);\n\n  let finalImage = img;\n\n  for (const item of boxesWithAnnotations) {\n    const { box, annotation } = item;\n    const conf = box[box.length - 1];\n\n    if (conf < threshold) continue;\n\n    const [xMin, yMin, xMax, yMax] = box;\n    const yOffset = 10;\n\n    finalImage = finalImage.drawRect(\n      xMin, yMin,\n      xMax - xMin, yMax - yMin,\n      { color: 'green', width: 3 },\n    );\n    finalImage = finalImage.drawText(\n      annotation,\n      xMin, yMin - yOffset,\n      { font: '30px Arial' },\n    );\n\n    if (!confLabels) continue;\n\n    finalImage = finalImage.drawText(\n      conf.toFixed(2),\n      xMin, yMax + 2 * yOffset,\n      { font: '20px Arial' },\n    );\n  }\n\n  return finalImage;\n}\n"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "### Do Inference and Show Detected Text Boxes and OCR Results for the Image\n"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "// Calculate ratios\nconst [ratioX, ratioY] = [img.width / detInputWidth, img.height / detInputHeight];\nconst boundingBoxesArray = extractBoundingBoxes(detResult[detOutputLayer]);\n// Resize bounding boxes to the original image size\nconst boundingBoxesOriginalSizeArray = boundingBoxesArray.map(box =>\n  [...multiplyByRatio(ratioX, ratioY, box), box[4]]);\n\n// Process each bounding box and run inference on the recognition model\nconst boxesWithAnnotations = [];\nfor (let i = 0; i < boundingBoxesOriginalSizeArray.length; i++) {\n  const box = boundingBoxesOriginalSizeArray[i];\n  const [xMin, yMin, xMax, yMax] = box;\n  const croppedImg = img.crop(xMin, yMin, xMax - xMin, yMax - yMin);\n  croppedImg.display(display);\n\n  const annotation = await performTextRecognition(recModel, recInferRequest, croppedImg);\n\n  boxesWithAnnotations.push({ box, annotation });\n\n  console.log(`Box ${i}: [${box.join(',')}], Annotation: '${annotation}'`);\n}\n"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "## Display the OCR Results on the original image"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "const annotatedImg = await putAnnotationsOnImage(\n  img,\n  boxesWithAnnotations,\n  { threshold: 0.3, confLabels: false },\n);\nannotatedImg.display(display);\n"
            ],
            "outputs": []
        }
    ]
}